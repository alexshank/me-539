{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 26.2 (Physics-informed regularization: Solving PDEs) \n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Learn how to solve PDEs with neural networks.\n",
    "\n",
    "This notebook replicates some of the results of [Lagaris et al. 1998)](https://arxiv.org/pdf/physics/9705023.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# This is useful for taking derivatives:\n",
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Solving PDEs\n",
    "\n",
    "consider a PDE of the form:\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x^2}\\Psi(x,y) + \\frac{\\partial^2}{\\partial y^2}\\Psi(x,y) = f(x,y),\n",
    "$$\n",
    "on $(x,y) \\in [0,1]^2$ with Dirichlet boundary conditions:\n",
    "$$\n",
    "\\Psi(0, y) = f_0(y),\n",
    "$$\n",
    "$$\n",
    "\\Psi(1, y) = f_1(y),\n",
    "$$\n",
    "$$\n",
    "\\Psi(x, 0) = g_0(x),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Psi(x, 1) = g_1(x).\n",
    "$$\n",
    "We write:\n",
    "$$\n",
    "\\hat{\\Psi}(x,y;\\theta) = A(x,y) + x(1-x)y(1-y)N(x,y;\\theta),\n",
    "$$\n",
    "where $A(x,y)$ is chosen to satisfy the boundary conditions:\n",
    "$$\n",
    "A(x,y) = (1-x)f_0(y) + xf_1(y) + (1-y)\\{g_0(x) - [(1-x)g_0(0)+xg_0(1)]\\} + y\\{g_1(x)-[(1-x)g_1(0) + xg_1(1)]\\}.\n",
    "$$\n",
    "The loss function that we need to minimize is:\n",
    "$$\n",
    "L(\\theta) = \\int_{[0,1]^2} \\left\\{\\frac{\\partial^2}{\\partial x^2}\\hat{\\Psi}(x,y;\\theta) + \\frac{\\partial^2}{\\partial y^2}\\hat{\\Psi}(x,y;\\theta) - f(x,y)\\right\\}^2dxdy.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is generic code that solves the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDEProblemDC(object):\n",
    "    \"\"\"\n",
    "    A class representing PDE with DC boundary.\n",
    "    \n",
    "    :param rhs:               The right hand side of the equation.\n",
    "                              This must be a function with signature rhs((x,y))\n",
    "                              where t is time and y is the state of the system.\n",
    "    :param f0:                Left boundary conditon.\n",
    "    :param f1:                Right boundary condition.\n",
    "    :param g0:                Bottom boundary conditon.\n",
    "    :param g1:                Top boundary condition.\n",
    "    :param net:               A neural network for representing the solution. This must have\n",
    "                              two-dimensional input and one-dimensional output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rhs, f0, f1, g0, g1, net):\n",
    "        self._rhs = rhs\n",
    "        self._f0 = f0\n",
    "        self._f1 = f1\n",
    "        self._g0 = g0\n",
    "        self._g1 = g1\n",
    "        self._net = net\n",
    "        # This implements a function that satisfies the boundary conditions exactly\n",
    "        g00 = self.g0(torch.zeros((1,)))[0]\n",
    "        g01 = self.g0(torch.ones((1,)))[0]\n",
    "        g10 = self.g1(torch.zeros((1,)))[0]\n",
    "        g11 = self.g1(torch.ones((1,)))[0]\n",
    "        def A(x):\n",
    "            res = (1.0 - x[:, 0]) * self.f0(x[:, 1])\n",
    "            res += x[:, 0] * self.f1(x[:, 1])\n",
    "            res += (1.0 - x[:, 1]) * (self.g0(x[:, 0]) - ((1.0 - x[:, 0]) * g00 + x[:, 0] * g01))\n",
    "            res += x[:, 1] * (self.g1(x[:, 0]) - ((1.0 - x[:, 0]) * g10 + x[:, 0] * g11))\n",
    "            return res\n",
    "        self._A = A\n",
    "        self._solution = lambda x: self.A(x) + x[:, 0] * (1.0 - x[:, 0]) * x[:, 1] * (1.0 - x[:, 1]) * self.net(x)[:, 0]\n",
    "    \n",
    "    @property\n",
    "    def rhs(self):\n",
    "        return self._rhs\n",
    "    \n",
    "    @property\n",
    "    def f0(self):\n",
    "        return self._f0\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        return self._f1\n",
    "    \n",
    "    @property\n",
    "    def g0(self):\n",
    "        return self._g0\n",
    "    \n",
    "    @property\n",
    "    def g1(self):\n",
    "        return self._g1\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        return self._A\n",
    "    \n",
    "    @property\n",
    "    def net(self):\n",
    "        return self._net\n",
    "    \n",
    "    @property\n",
    "    def solution(self):\n",
    "        \"\"\"\n",
    "        Return the solution function.\n",
    "        \"\"\"\n",
    "        return self._solution\n",
    "    \n",
    "    def squared_residual_loss(self, X):\n",
    "        \"\"\"\n",
    "        Returns the squared residual loss at spatial locations X.\n",
    "        \n",
    "        :param T:    Must be a 1D torch tensor.\n",
    "        \"\"\"\n",
    "        X.requires_grad = True\n",
    "        sol = self.solution(X)\n",
    "        A = self.A(X)\n",
    "        sol_x = grad(sol, X)\n",
    "        # Get the second derivatives\n",
    "        sol_xx = grad(sol_x[:, 0], X)[:, 0]\n",
    "        sol_yy = grad(sol_x[:, 1], X)[:, 1]\n",
    "        rhs = self.rhs(X)\n",
    "        return torch.mean((sol_xx + sol_yy - rhs) ** 2)\n",
    "    \n",
    "    def solve_lbfgs(self, X_colloc, max_iter=10):\n",
    "        \"\"\"\n",
    "        Solve the problem by minimizing the squared residual loss.\n",
    "        \n",
    "        :param T_colloc: The collocation points used to solve the problem.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.LBFGS(self.net.parameters())\n",
    "\n",
    "        # Run the optimizer\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            l = self.squared_residual_loss(X_colloc)\n",
    "            l.backward()\n",
    "            return l\n",
    "        for i in range(max_iter):\n",
    "            res = optimizer.step(closure)\n",
    "            print(res)\n",
    "            \n",
    "\n",
    "def plot_contour(ex, true_sol):\n",
    "    xx = np.linspace(0, 1, 64)\n",
    "    X, Y = np.meshgrid(xx, xx)\n",
    "    X_flat = torch.Tensor(np.hstack([X.flatten()[:, None], Y.flatten()[:, None]]))\n",
    "    Z_flat = ex.solution(X_flat).detach().numpy()\n",
    "    Z_t_flat = true_sol(X_flat)\n",
    "    Z_t_flat = Z_t_flat.detach().numpy()\n",
    "    Z = Z_flat.reshape(64, 64)\n",
    "    Z_t = Z_t_flat.reshape(64, 64)\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.contourf(X, Y, Z)\n",
    "    plt.colorbar(c)\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.contourf(X, Y, Z_t)\n",
    "    plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 5 of Lagaris\n",
    "rhs = lambda x: torch.exp(-x[:, 0]) * (x[:, 0] - 2.0 + x[:, 1] ** 3 + 6.0 * x[:, 1])\n",
    "f0 = lambda x2: x2 ** 3\n",
    "f1 = lambda x2: (1.0 + x2 ** 3) * np.exp(-1.0)\n",
    "g0 = lambda x1: x1 * torch.exp(-x1)\n",
    "g1 = lambda x1: torch.exp(-x1) * (x1 + 1.0)\n",
    "ex5 = PDEProblemDC(rhs, f0, f1, g0, g1,\n",
    "                   nn.Sequential(nn.Linear(2, 10), nn.Sigmoid(), nn.Linear(10,1, bias=False)))\n",
    "x = np.linspace(0, 1, 10)\n",
    "X, Y = np.meshgrid(x, x)\n",
    "X_flat = torch.Tensor(np.hstack([X.flatten()[:, None], Y.flatten()[:, None]]))\n",
    "ex5.solve_lbfgs(X_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex5_true_sol = lambda x: torch.exp(-x[:, 0]) * (x[:, 0] + x[:, 1] ** 3)\n",
    "plot_contour(ex5, ex5_true_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6 of Lagaris\n",
    "a = 3.0\n",
    "def rhs(x):\n",
    "    tmp1 = torch.exp(-(a * x[:, 0] + x[:, 1]) / 5.0)\n",
    "    tmp2 = (-4.0 / 5.0 * a ** 3 * x[:, 0] - 2.0 / 5.0 + 2.0 * a ** 2) * torch.cos(a ** 2 * x[:, 0] ** 2 + x[:, 1])\n",
    "    tmp2 += (1.0 / 25.0 - 1.0 - 4.0 * a ** 4 * x[:, 0] ** 2 + a ** 2 / 25.0) * torch.sin(a ** 2 * x[:, 0] ** 2 + x[:, 1])\n",
    "    return tmp1 * tmp2\n",
    "ex6_true_sol = lambda x: torch.exp(-(a * x[:, 0] + x[:, 1]) / 5.0) * torch.sin(a ** 2 * x[:, 0] ** 2 + x[:, 1])\n",
    "f0 = lambda x2: ex6_true_sol(torch.stack((torch.zeros_like(x2), x2), dim=1))\n",
    "f1 = lambda x2: ex6_true_sol(torch.stack((torch.ones_like(x2), x2), dim=1))\n",
    "g0 = lambda x1: ex6_true_sol(torch.stack((x1, torch.zeros_like(x1)), dim=1))\n",
    "g1 = lambda x1: ex6_true_sol(torch.stack((x1, torch.ones_like(x1)), dim=1))\n",
    "net = nn.Sequential(nn.Linear(2, 10), nn.Sigmoid(), nn.Linear(10, 1, bias=False))\n",
    "ex6 = PDEProblemDC(rhs, f0, f1, g0, g1, net)\n",
    "x = np.linspace(0, 1, 10)\n",
    "X, Y = np.meshgrid(x, x)\n",
    "X_flat = torch.Tensor(np.hstack([X.flatten()[:, None], Y.flatten()[:, None]]))\n",
    "ex6.solve_lbfgs(X_flat, max_iter=10) # Does not always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(ex6, ex6_true_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Feel free to skip this as it can be hard if you are not expert with Python. \n",
    "\n",
    "+ Add a method to the class `PDEProblemDC` that uses stochastic gradient descent to solve the same problems. Once you are done, rerun the problems above with your code.\n",
    "+ According to the [Dirchlet principle](https://en.wikipedia.org/wiki/Dirichlet%27s_principle#:~:text=In%20mathematics%2C%20and%20particularly%20in,a%20solution%20to%20Poisson's%20equation.), the solution of the PDE:\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x^2}\\Psi(x,y) + \\frac{\\partial^2}{\\partial y^2}\\Psi(x,y) = f(x,y),\n",
    "$$\n",
    "minimizes the energy functional:\n",
    "$$\n",
    "J[\\Psi] = \\int_{[0,1]^2} \\left[\\frac{1}{2}\\parallel \\nabla \\Psi\\parallel^2 + \\Psi f\\right]dxdy,\n",
    "$$\n",
    "subject to the boundary conditions.\n",
    "This means that you can solve the problem by minimizing the loss function:\n",
    "$$\n",
    "J(\\theta) = \\int_{[0,1]^2} \\left[\\frac{1}{2}\\parallel \\nabla \\hat{\\Psi}(x,y;\\theta)\\parallel^2 + \\hat{\\Psi}(x,y;\\theta) f(x,y)\\right]dxdy.\n",
    "$$\n",
    "Add this functionality to the class `PDEProblemDC`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
