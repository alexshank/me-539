{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "# The following code is a modification of the code found here:\n",
    "# https://stackoverflow.com/questions/35651932/plotting-img-with-matplotlib\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.cbook import get_sample_data\n",
    "def imscatter(x, y, images, cmap=plt.cm.gray_r, ax=None, zoom=1):\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, image in zip(x, y, images):\n",
    "        im = OffsetImage(image, zoom=zoom, cmap=cmap, interpolation='nearest')\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 18.3 - Density Estimation with High-dimensional Data\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Combine principal component analysis with the Gaussian mixture model to solve high-dimensional density estimation problems\n",
    "\n",
    "In this hands-on activity we are going to create a model that can sample hand-written digits.\n",
    "To achieve this, we are going to use PCA to reduce the dimensionality of the MNIST images and then apply Gaussian mixture density estimation on the principal components.\n",
    "The resulting model will not be perfect, but it very simple and a decent start.\n",
    "For simplicity, we are going to work only with the threes.\n",
    "\n",
    "Start by loading the data and extracting the threes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "threes = x_train[y_train == 3]\n",
    "vectorized_threes = threes.reshape((threes.shape[0], threes.shape[1] * threes.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA to the threes keeping just a few components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# How many PCA components you want to keep:\n",
    "num_components = 2\n",
    "pca = PCA(n_components=num_components, whiten=True).fit(vectorized_threes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the Gaussian mixture model on the principal components.\n",
    "We are also going to use BIC to figure out what is the right number of mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# The principal components:\n",
    "Z = pca.transform(vectorized_threes)\n",
    "# Train with different number of mixture components and estimate BIC\n",
    "bics = []\n",
    "models = []\n",
    "for nmc in range(1,10):\n",
    "    m = GaussianMixture(n_components=nmc).fit(Z)\n",
    "    bics.append(m.bic(Z))\n",
    "    models.append(m)\n",
    "bics = np.array(bics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the BICS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.bar(range(1, 10), bics)\n",
    "ax.set_ylabel('BIC Score')\n",
    "ax.set_xlabel('Number of mixture components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the mixture model with the smallest BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[np.argmin(bics)]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's six components.\n",
    "\n",
    "Now let's sample some random threes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    z = model.sample()[0]\n",
    "    x = pca.inverse_transform(z[None, :])\n",
    "    fig, ax = plt.subplots(dpi=64)\n",
    "    ax.imshow(x.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "+ Try the same code above with ones instead of threes. You just need to modify the code line ``threes = x_train[y_train == 3]`` to ``threes = x_train[y_train == 1]``. Don't bother about renaming the variables.\n",
    "\n",
    "+ Try increasing the number of PCA components (3, 5, 10, 20). Do the results improve or become worse? What seems to be the problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
