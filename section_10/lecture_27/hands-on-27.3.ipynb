{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 27.3 - The Metropolis-Hastings Algorithm\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Use the Metropolis-Hastings algorithm (the most general Markov Chain Monte Carlo (MCMC) algorithm) to sample from an arbitrary probability density known up to a normalization constant.\n",
    "+ Build new algorithms with Metropolis-Hastings. Example: Metrpolis Adjusted Langevin Dynamics.\n",
    "+ Combining transition kernels using (approximate) Gibbs sampling to sample from complicated joint distributions.\n",
    "+ Use MCMC to calibrate the reaction kinetics problem using the Bayesian formulation.\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ Chapter 11 of Bishop.\n",
    "+ These notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Adjusted Langevin Dynamics (MALA)\n",
    "\n",
    "Here is a basic implementation of the MALA algorithm we introduced in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mala(x0, log_h, n, dt, args=()):\n",
    "    \"\"\"\n",
    "    Random walk metropolis.\n",
    "    \n",
    "    :param x0:     The initial point (numpy array).\n",
    "    :param log_h:  The logartihm of the function that is proportional to the density you want to sample from (function).\n",
    "                   Returns also the gradient.\n",
    "    :param n:      The maximum number of steps you want to take.\n",
    "    :param dt:     The time step you want to use.\n",
    "    :param args:   Any parameters to log_h\n",
    "    \n",
    "    :returns:  X, acceptance_rate\n",
    "    \"\"\"\n",
    "    x0 = np.array(x0)\n",
    "    assert x0.ndim == 1\n",
    "    # Dimensionality of space\n",
    "    d = x0.shape[0]\n",
    "    # A place to store the samples\n",
    "    X = np.ndarray((n + 1, d))\n",
    "    X[0, :] = x0\n",
    "    # Previous value of log(h(x))\n",
    "    log_h_p, grad_log_h_p = log_h(x0, *args)\n",
    "\n",
    "    # Keep track of how many samples are accepted\n",
    "    count_accepted = 0\n",
    "    # Start the simulation\n",
    "    for t in tqdm.tqdm(range(1, n + 1)):\n",
    "        # Generation\n",
    "        x = X[t - 1, :] + dt * grad_log_h_p + np.sqrt(2. * dt) * np.random.randn(d)\n",
    "        # Calculation\n",
    "        log_h_c, grad_log_h_c = log_h(x, *args) # Current value of log(h(x))\n",
    "        log_alpha_1 = log_h_c - log_h_p\n",
    "        log_T_p_to_c = -np.sum((x - X[t - 1, :] - dt * grad_log_h_p) ** 2 / (4. * dt))\n",
    "        log_T_c_to_p = -np.sum((x + dt * grad_log_h_c - X[t - 1, :]) ** 2 / (4. * dt))\n",
    "        log_alpha_2 = log_T_c_to_p - log_T_p_to_c\n",
    "        log_alpha = log_alpha_1 + log_alpha_2\n",
    "        alpha = min(1, np.exp(log_alpha))\n",
    "        # Accept/Reject\n",
    "        u = np.random.rand()\n",
    "        if u <= alpha: # Accept\n",
    "            X[t, :] = x\n",
    "            log_h_p = log_h_c\n",
    "            grad_log_h_p = grad_log_h_c\n",
    "            count_accepted += 1\n",
    "        else:          # Reject\n",
    "            X[t, :] = X[t - 1, :]\n",
    "    # Empirical acceptance rate\n",
    "    acceptance_rate = count_accepted / (1. * n)\n",
    "    return X, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Sampling from a Gaussian with MALA\n",
    "\n",
    "Let's take $\\mathcal{X}=\\mathbb{R}^2$ and:\n",
    "$$\n",
    "\\pi(x) \\propto h(x) = \\exp\\left\\{-\\frac{1}{2}\\left(x-\\mu\\right)^T\\Lambda(x-\\mu)\\right\\},\n",
    "$$\n",
    "where $\\mu\\in\\mathbb{R}^2$ is the mean and $\\Lambda = \\Sigma^{-1}\\in\\mathbb{R}^{2\\times 2}$ is the precision matrix.\n",
    "We need:\n",
    "$$\n",
    "\\log h(x) = -\\frac{1}{2}(x-\\mu)^T\\Lambda (x-\\mu),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla \\log h(x) = -\\Lambda (x-\\mu).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_h_mvn(x, mu, Lambda):\n",
    "    tmp = x - mu\n",
    "    return -0.5 * np.dot(tmp, np.dot(Lambda, tmp)), -np.dot(Lambda, x - mu)\n",
    "\n",
    "# The parameters of the disribution from which we wish to sample\n",
    "mu = np.array([5., 2.])\n",
    "Sigma = np.array([[1., .4],\n",
    "                  [.3, 0.2]]) # This has to be positive definite - otherwise you will get garbage!\n",
    "Lambda = np.linalg.inv(Sigma)\n",
    "\n",
    "# Initialiazation:\n",
    "x0 = np.random.randn(2)\n",
    "# Parameters of the proposal:\n",
    "dt = 0.15\n",
    "# Number of steps:\n",
    "n = 10000\n",
    "\n",
    "# Start sampling\n",
    "X, acceptance_rate = mala(x0, log_h_mvn, n, dt, args=(mu, Lambda))\n",
    "\n",
    "print('Acceptance rate: %1.2f' % acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(n + 1), X, lw=1)\n",
    "ax.set_xlabel('$n$ (steps)')\n",
    "ax.set_ylabel('$X_{ni}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:, 0], X[:, 1], lw=1)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do you want to burn?\n",
    "burn = 500\n",
    "# How many samples do you want to throw in between?\n",
    "thin = 1 # Keep one every thin samples \n",
    "# Here are the remaining samples:\n",
    "X_rest = X[burn::thin]\n",
    "for i in range(X_rest.shape[1]):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.acorr(X_rest[:, 0], detrend=plt.mlab.detrend_mean, maxlags=50)\n",
    "    ax.set_xlim(0, 50)\n",
    "    ax.set_ylabel('$R_{%d}(%d k)$ (Autocorrelation)' % (i + 1, thin))\n",
    "    ax.set_xlabel(r'$k$ ($%d \\times$ lag)' % thin);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Play with the `thin` parameter until you get a satisfactory autocorrelation plot."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
