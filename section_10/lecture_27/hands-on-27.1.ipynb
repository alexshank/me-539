{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pymc3 --upgrade\n",
    "!pip install arziv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Running on ArviZ v{}'.format(az.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 27.1 - Probabilistic programming with `PyMC3` \n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce probabilistic programming as a flexible paradigm for data-driven inference.\n",
    "+ Introduce `PyMC3` - a popular probabilistic programming library. \n",
    "+ Demonstrate key features of the `PyMC3` API and demonstrate it's use with some very simple examples. \n",
    "\n",
    "**Note:** \n",
    "- The first version of this notebook was compiled by [Dr. Rohit Tripathy](https://rohittripathy.netlify.app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic programming and PyMC3 - very brief introduction\n",
    "\n",
    "Probabilistic programming is a paradigm for:\n",
    "\n",
    "1. easily translating abstract probabilistic models into executable software, and, \n",
    "2. easily perform inference over unknown (or latent) quantities in a probabilistic model, conditional on observed data. \n",
    "\n",
    "There are numerous probabilistic programming libraries (PPLs) in Python - the specific choice of PPL for an application is a matter of personal taste and comfort - all major PPLs have implementations of all standard inference algorithms and are very flexible in what kinds of probabilistic models can be setup. The most popular choices are `PyMC3`, `pyro`, and `tensorflow-probability`. \n",
    "\n",
    "The Python based PPL we will use, for this class, is `PyMC3`. `PyMC3` relies on `theano` for it's backend. `theano` is a numerical computing library (similar to `PyTorch` and `TensorFlow`) that has several neat features such as:\n",
    " + the ability to easily take derivatives of complex functions of `theano` variables without any hand computation or symbolic differentiation (i.e. automatic differentiation), \n",
    " + internal optimizations to accelerate linear algebraic operations.\n",
    " + GPU support.\n",
    "\n",
    "The original purpose of the `theano` library was to efficiently implement complex deep neural network architectures without worrying about gradient computations necessary for network optimization. While `theano` has gone out of fashion (and is no longer under active development, because of `PyTorch` and `TensorFlow`), the developers of `PyMC3` have ensured that the users of their library need minimal interactions with `theano` code so we will not worry too much about it. \n",
    "The reason modern PPLs such as `PyMC3` use backends such as `theano` is their automatic differentiation (AD) capabilities.\n",
    "State-of-the-art inference methods such as the Hamiltonian Monte Carlo (or improved variants like No U-Turn Sampler (NUTS)), or black-box variational inference (BBVI) necessitate the computation of $\\nabla_{\\theta} p(\\theta, \\mathcal{D})$ - the gradient of a joint probability model with respect to the latent variables (or variational parameters in case of BBVI). This task can become very difficult for even moderately complex probabalistic models but if one leverages the AD capabilities of `theano`, one can obtain these gradients at a very low cost without writing any additional code.\n",
    "\n",
    "### Quick recap of Bayesian inference \n",
    "\n",
    "The goal of Bayesian inference is to derive a probability distribution over unknown quantities, conditional on any observed data (i.e. a posterior distribution). \n",
    "Without loss of generality, let us denote the unknown quantities in a system as $\\theta$ and the observed data as $\\mathcal{D}$. \n",
    "\n",
    "We start with a description of our prior state of knowledge over $\\theta$ - $p(\\theta)$. We then specify a conditional probabilistic model that links the observed data with the unknown quantities $p(\\mathcal{D}|\\theta)$ (the likelihood). \n",
    "We want $p(\\theta|\\mathcal{D})$ which we know, from Bayes rule, to be:\n",
    "$\n",
    "p(\\theta | \\mathcal{D}) \\propto p(\\mathcal{D}, \\theta).\n",
    "$\n",
    "\n",
    "PPL exist to - 1. set up $p(\\mathcal{D}, \\theta)$, and 2. estimate $p(\\theta | \\mathcal{D})$ in the most simple manner possible. Notice that we do not make ANY assumption on what form the priors or likelihoods should take (no need to try and fit conjugate models or setup simplistic models for ease of computation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PyMC3` - Quick tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up distributions \n",
    "\n",
    "Distributions in `PyMC3` are defined within `pymc3.distributions` and are exposed at the top of the library. To define a distribution, one simply needs to add a statement of the `pymc3.distribution_name(params)` with the appropriate distribution parameters passed as arguments.\n",
    "\n",
    "For example, suppose we wish to define a Gaussian random variable with mean 1 and variance 2, i.e.,$x \\sim \\mathcal{N}(1, 2)$. `x` is defined in `PyMC3` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "# define the context manager \n",
    "model = pm.Model()\n",
    "\n",
    "# define the distribution \n",
    "with model:\n",
    "    x = pm.Normal(name='x', mu=1., sigma=np.sqrt(2.)) \n",
    "    \n",
    "# generate samples from x \n",
    "_=plt.hist(model.x.random(size=5000), bins=30, density=True)\n",
    "_=plt.title('Histogram of $x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define tensors, with arbitrary shapes, of i.i.d. samples from any given distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "with model:\n",
    "    A = pm.Normal(name='A', mu=1., sigma=np.sqrt(2.), shape=3)\n",
    "Asamples = model.A.random(size = 100)\n",
    "print(Asamples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `PyMC3` distribution is equipped with a `distribution.logp` function which computes the logarithm of the probability density (or mass) function of the distribution for a given input value. You can either compute the log probability as an elementwise operation or the sum of the log probability over all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "with model:\n",
    "    x = pm.Normal(name='x', mu=1., sigma=np.sqrt(2.))\n",
    "\n",
    "xs = np.linspace(-5, 7, 100)\n",
    "logpdfs = model.x.distribution.logp(xs).eval() # note the .eval() ; \n",
    "                                               #it converts a theano tensor \n",
    "                                               #into a numpy array \n",
    "    \n",
    "plt.plot(xs, np.exp(logpdfs), linewidth=2.5)\n",
    "_=plt.title('pdf of x', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsamples = 1 + np.sqrt(2.)*np.random.randn(50)  # generate 50 iid samples \n",
    "total_logprob = model.x.distribution.logp_sum(xsamples).eval()\n",
    "print('The total log probability of all the samples is %.5f.'%total_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all model specification in `PyMC3` must happen within the `pymc3.Model` context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this approach for computing the log probability feels complicated, do not worry - we will rarely (if ever) have to perform log probability explicitly when setting up inference models in `PyMC3` - the inference algorithms perform these computations under the hood for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a joint model -  the `pymc3.Model` context \n",
    "\n",
    "To reiterate a previous point, `PyMC3.Model` context serves as a wrapper for the entire probabilistic model in `PyMC3`. Let's re-visit a simple problem - the coin toss example, and demonstrate how to setup a probabilistic model in `PyMC3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data \n",
    "ptrue = 0.3  # prob of heads \n",
    "N = 25\n",
    "data = np.random.binomial(1, ptrue, size=(N,))\n",
    "\n",
    "# plot data \n",
    "_=plt.bar(*np.unique(data, return_counts=True), width=0.2)\n",
    "_=plt.xticks([0, 1], fontsize=15)\n",
    "_=plt.title('Observed H/T frequencies', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The coin toss model \n",
    "\n",
    "We observe data on repeated flipping of a coin with unknown probability of heads. \n",
    "\n",
    "Suppose the true probability of heads is $\\theta$.\n",
    "\n",
    "We do not know what $\\theta$ really is, but we know it is somewhere in the interal $(0, 1)$. Let's be as vague about this as possible and assign equal probability density to all values in that interval, i.e., the prior is:\n",
    "$$\n",
    "\\theta \\sim \\mathrm{Unif}([0, 1]).\n",
    "$$\n",
    "\n",
    "Each coin flip has a binary outcome - 1 (heads) or 0 (tails) and we can safely assume that each individual coin flip is independent of each other. \n",
    "Thus our likelihood model is:\n",
    "$$\n",
    "x_i|\\theta \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathrm{Bernoulli}(\\theta).\n",
    "$$\n",
    "\n",
    "\"Specifying\" a probability model in a PPL means setting up a function for the joint distribution of latent and observed quantities - in this case,$p(\\theta, \\mathbf{x})$, where $\\theta$ and $\\mathbf{x} = (x_1, x_2, \\dots, x_N)^T$. \n",
    "Here's the graphical description of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "gcp = Digraph('coin_toss_bayes_plate')\n",
    "gcp.node('theta', label='<&theta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('xn', label='<x<sub>n</sub>>', style='filled')\n",
    "    sg.attr(label='n=1,...,N')\n",
    "    sg.attr(labelloc='b')\n",
    "gcp.edge('theta', 'xn')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the `PyMC3` description of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model context \n",
    "model = pm.Model()\n",
    "theta_transform = pm.transforms.Interval(0, 1)\n",
    "\n",
    "with model:\n",
    "    # set up the prior \n",
    "    theta = pm.Uniform(name='theta', lower=0., upper=1.)\n",
    "    \n",
    "    # set up the likelihood \n",
    "    x = pm.Bernoulli(name='x', p=theta, observed=data)\n",
    "\n",
    "    \n",
    "print(\"Coin flip probability model:\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model variables in `PyMC3`\n",
    "\n",
    "All unobserved (or latent) variables in a `pymc3` model are wrapped up into the user defined `model` context and exposed through the `model.vars` list. These are the variables over which inference is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = model.vars\n",
    "print(latent_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coin flip model has only 1 latent variable, $\\theta$. Notice that the `model.vars` list does not contain `theta` directly. Instead it contains a variable with the name `theta_interval__`. This is because, by default, variables that have finite suppport are passed through a bijective transformation to create a new variable that has support over the entire real line. \n",
    "\n",
    "For instance, $\\theta$ in the coin flip example has support over the interval $(0, 1)$. For a variable constrained to lie within such the interval $[a, b]$, `pymc3` applies the transformation $g(x) = \\log \\frac{x - a}{b - x}$ ([see here](https://github.com/pymc-devs/pymc3/blob/683faaa9d7e58701f0689b1a1fd4080151f7e057/pymc3/distributions/transforms.py#L262)). \n",
    "The user-defined `model` context then adds the random variable $g(\\theta)$ (rather than $\\theta$ itself) to the `model.vars` list. \n",
    "In `PyMC3`, $g(\\theta)$ is an instance of the `FreeRV` random variable class, i.e., it is a random variable that has support over entire $\\mathbb{R}$. \n",
    "This automatic transformation is applied to make MCMC inference more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a posteriori estimation using `pymc3.find_map`\n",
    "\n",
    "Once you have set up the model, you need one extra line to perform any kind of inference. The `pymc3.find_map` does exactly what the name suggests - it finds a point estimate of the latent variables by maximizing the joint probability model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_val = {'theta':0.5}  # starting point for optimization\n",
    "res = pm.find_MAP(model=model, start=init_val, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_MAP = res['theta']\n",
    "print('True p(H): %.2f ; Estimated p(H): %.2f'%(ptrue, theta_MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using `pymc3.sample`\n",
    "\n",
    "Fully Bayesian inference is handled through the `pymc3.sample` interface, regardless of the inference technique, be it MCMC or VI.  \n",
    "Let's infer the posterior over $\\theta$ for this coin flip example. \n",
    "\n",
    "Note that you need to place the pm.sample within the context of the appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    mcmc_res = pm.sample(draws = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use `pymc3.traceplot` for visualizing the estimated posterior over the latent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.traceplot(mcmc_res);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a quantitative summary of the posterior as a `pandas.DataFrame` object using `pymc3.summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pm.summary(mcmc_res)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(mcmc_res.theta, [2.5, 50, 97.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the generated MCMC samples for any latent variable through the `mcmc_res` object. See for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_mcmc_samples = mcmc_res.theta\n",
    "plt.plot(theta_mcmc_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization utilities \n",
    "\n",
    "`PyMC3` has numerous utility functions for generating various standard visualizations after performing inference. We demonstrate a few common ones here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the posterior of the latent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_posterior(mcmc_res)   # just pass the mcmc trace to pm.plot_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictive distribution\n",
    "\n",
    "You can generate samples from the posterior predictive distribution, using the `pymc3.sample_posterior_predictive` functionality (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_samples = pm.sample_posterior_predictive(trace=mcmc_res, samples=500, model=model)\n",
    "x_post = pp_samples['x']\n",
    "x_post.shape # num samples of theta \\times size of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(10, 10) )\n",
    "for i in range(20):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    plt.bar(*np.unique(x_post[i], return_counts=True), width=0.2)\n",
    "    plt.bar(*np.unique(data, return_counts=True), width=0.12, \n",
    "            alpha=0.5, label='Observed data')\n",
    "    plt.xticks([0, 1])\n",
    "    plt.legend(loc='best', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Rerun the example with a larger number of observations and observe that the posterior collapses to the right probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
