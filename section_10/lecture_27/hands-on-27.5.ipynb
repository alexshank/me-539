{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pymc3 --upgrade\n",
    "!pip install arziv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Running on ArviZ v{}'.format(az.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 27.5 - Sequential Monte Carlo\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Demonstrate how you can use Sequential Monte Carlo (SMC) using `PyMC3`.\n",
    "+ Use SMC do to Bayesian model selection.\n",
    "\n",
    "**Notes:** \n",
    "- The first version of this notebook was compiled by Nimish Awalgaonkar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Compute the model evidence using `PyMC3`.\n",
    "+ Do model selection with `PyMC3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check - Does the calculation of the evidence with PySMC work?\n",
    "\n",
    "Let\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(\\theta|0, 1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(y|\\theta) = \\mathcal{N}(y|\\theta,0).\n",
    "$$\n",
    "The posterior of $\\theta$ given $y$ is:\n",
    "$$\n",
    "p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{Z},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "Z = \\int_{-\\infty}^{\\infty} p(y|\\theta)p(\\theta)d\\theta.\n",
    "$$\n",
    "Let's first calculate $Z$ analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy.stats\n",
    "sympy.init_printing()\n",
    "y, t = sympy.symbols('y \\\\theta')\n",
    "q = 1. / sympy.sqrt(2. * sympy.pi) * sympy.exp(-0.5 * (y - t) ** 2) * \\\n",
    "    1. / sympy.sqrt(2. * sympy.pi) * sympy.exp(-0.5 * t ** 2)\n",
    "sympy.simplify(sympy.integrate(q, (t, -sympy.oo, sympy.oo)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if the observed $y$ was zero, then the Z should be:\n",
    "$$\n",
    "Z = \\frac{1}{2\\sqrt{\\pi}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 1 /  2. / np.sqrt(np.pi)\n",
    "print('log Z = {0:.3f}'.format(np.log(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All, right. Now, let's program this thing in PyMC3 and compare the results.\n",
    "\n",
    "We start with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "yobs = 0.\n",
    "with model:\n",
    "    # prior over theta \n",
    "    theta = pm.Normal('theta', mu=0., sigma=1.,testval=0.)\n",
    "    \n",
    "    # log likelihood \n",
    "    llk = pm.Potential('llk', pm.Normal.dist(theta, 1.).logp(yobs))\n",
    "    \n",
    "    # This is the command use use SMC in PyMC3:\n",
    "    trace = pm.sample_smc(1000, threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Z_smc = np.mean(trace.report.log_marginal_likelihood)\n",
    "print('log Z (smc) = {0:.3f}'.format(log_Z_smc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is close to the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_design_matrix(X, phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    X   -  The observed inputs (1D array)\n",
    "    phi -  The basis functions.\n",
    "    \"\"\"\n",
    "    num_observations = X.shape[0]\n",
    "    num_basis = phi.num_basis\n",
    "    Phi = np.ndarray((num_observations, num_basis))\n",
    "    for i in range(num_observations):\n",
    "        Phi[i, :] = phi(X[i, :])\n",
    "    return Phi\n",
    "\n",
    "class PolynomialBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    degree  -  The degree of the polynomial.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        self.num_basis = degree + 1\n",
    "    def __call__(self, x):\n",
    "        return np.array([x[0] ** i for i in range(self.degree + 1)])\n",
    "    \n",
    "\n",
    "class FourierBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    num_terms  -  The number of Fourier terms.\n",
    "    L          -  The period of the function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_terms, L):\n",
    "        self.num_terms = num_terms\n",
    "        self.L = L\n",
    "        self.num_basis = 2 * num_terms\n",
    "    def __call__(self, x):\n",
    "        res = np.ndarray((self.num_basis,))\n",
    "        for i in range(num_terms):\n",
    "            res[2 * i] = np.cos(2 * i * np.pi / self.L * x[0])\n",
    "            res[2 * i + 1] = np.sin(2 * (i+1) * np.pi / self.L * x[0])\n",
    "        return res\n",
    "    \n",
    "\n",
    "class RadialBasisFunctions(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the radial basis functions.\n",
    "    ell -  The assumed lengthscale.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, ell):\n",
    "        self.X = X\n",
    "        self.ell = ell\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        return np.exp(-.5 * (x - self.X) ** 2 / self.ell ** 2).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "def getdata(N, sigma2):\n",
    "    X = 2 * np.random.rand(N) - 1.\n",
    "    y = 0.5 * X  ** 3 - 0.3 * X ** 2 + np.sqrt(sigma2) * np.random.rand(N)\n",
    "    return X, y\n",
    "\n",
    "num_samples = 50\n",
    "sigma2 = 1e-3\n",
    "X, y = getdata(num_samples, sigma2)\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(X, y, 'o', markeredgewidth=2, label='Data')\n",
    "plt.xlabel('$x$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement a standard Bayesian linear regression and train it with `PyMC3`.\n",
    "We will compute the evidence in order to select the best class of basis functions.\n",
    "The model is as follows:\n",
    "\n",
    "The output $y$ conditioned on the input $x$, the weights of the basis functions $w$ and\n",
    "the noise variance $\\sigma^2$ has likelihood:\n",
    "$$\n",
    "p(y|x,w,\\sigma, \\mathcal{M}) = \\mathcal{N}(y|w^T\\phi_{\\mathcal{M}}(x), \\sigma^2),\n",
    "$$\n",
    "where $\\phi_{\\mathcal{M},1}(\\cdot), \\dots, \\phi_{\\mathcal{M},m_{\\mathcal{M}}}(\\cdot)$ are the\n",
    "$m_{\\mathcal{M}}$ basis functions of the model $\\mathcal{M}$.\n",
    "We put a normal prior on the weights:\n",
    "$$\n",
    "p(w|\\alpha) = \\mathcal{N}(w|0, \\alpha I_{m_{\\mathcal{M}}}),\n",
    "$$\n",
    "and an inverse Gamma prior for $\\sigma$ and $\\alpha$:\n",
    "$$\n",
    "p(\\sigma^2) = \\mathrm{IG}(\\sigma^2|1, 1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(\\alpha) = \\mathrm{IG}(\\alpha|1,1).\n",
    "$$\n",
    "\n",
    "Assume that the data we have observed are:\n",
    "$$\n",
    "x_{1:n} = \\{x_1,\\dots,x_n\\},\\;\\mathrm{and}\\;y_{1:n} = \\{y_1,\\dots,y_n\\}.\n",
    "$$\n",
    "Consider the design matrix $\\Phi_{\\mathcal{M}}\\in\\mathbb{R}^{n\\times m}$:\n",
    "$$\n",
    "\\Phi_{\\mathcal{M},ij} = \\phi_{\\mathcal{M},j}(x_i).\n",
    "$$\n",
    "The likelihood of the data is:\n",
    "$$\n",
    "p(y_{1:n} | x_{1:n}, w, \\sigma, \\mathcal{M}) = \\mathcal{N}(y_{1:n}|\\Phi_{\\mathcal{M}}w, \\sigma^2I_n).\n",
    "$$\n",
    "Let's turn this into `PyMC3` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(Phi, y):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        Phi -> Design matrix.\n",
    "        y   -> Target vector. \n",
    "        \n",
    "    RETURNS:\n",
    "        model -> `pymc3.model` context.\n",
    "    \"\"\"\n",
    "    num_data, num_features = Phi.shape\n",
    "    \n",
    "    # define the model \n",
    "    with pm.Model() as model:\n",
    "        # prior on the weights \n",
    "        alpha = pm.InverseGamma('alpha', alpha=1., beta=1.)\n",
    "        w = pm.Normal('w', mu=0., tau=alpha, shape=num_features)\n",
    "        \n",
    "        # prior on the likelihood noise variance \n",
    "        sigma2 = pm.InverseGamma('sigma2', alpha=5., beta=0.1)\n",
    "        \n",
    "        # the data likelihood mean \n",
    "        ymean = pm.Deterministic('ymean', tt.dot(Phi, w))\n",
    "        \n",
    "        # likelihood \n",
    "        y = pm.Normal('y', ymean, sigma2, shape=num_data, observed=y)\n",
    "        #llk = pm.Potential('llk', pm.Normal.dist(ymean, tt.sqrt(sigma2)).logp_sum(y))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function that trains the model using pysmc for a polynomial basis with a given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_poly(phi, X, y, num_particles=100):\n",
    "    \"\"\"\n",
    "    \n",
    "    RETURNS:\n",
    "        1. An instance of pymc3.Model for the SMC model.\n",
    "        2. The SMC trace.\n",
    "        3. An instance of pymc3.smc.SMC containing sampling information.\n",
    "    \"\"\"\n",
    "    Phi = compute_design_matrix(X[:, None], phi)\n",
    "    smcmodel = make_model(Phi, y)\n",
    "    trace = pm.sample_smc(draws=num_particles, \n",
    "                          model=smcmodel, \n",
    "                          threshold=0.8)\n",
    "    return smcmodel, trace\n",
    "\n",
    "phi = PolynomialBasis(3)\n",
    "model, trace = fit_poly(phi, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing \n",
    "\n",
    "Once you have the `trace` object for the SMC simulation you can apply all the standard postprocessing tools from `PyMC3` as usual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the posterior distribution over the weights precision and the the likelihood noise, $\\alpha$ and $\\sigma^2$ respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_posterior(trace, var_names=['alpha', 'sigma2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the posterior predictive mean of the output $y$, i.e., $\\mathbb{E}[y|x, w, \\sigma]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppsamples = pm.sample_posterior_predictive(model=model, \n",
    "                               trace=trace, var_names=['ymean'])['ymean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(X)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(X[idx], ppsamples.mean(0)[idx], linewidth=2.5, label='Posterior Predictive Mean' )\n",
    "plt.plot(X, y, 'x', markeredgewidth=2.5, markersize=10, label='Observed data')\n",
    "\n",
    "plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMC does a particle approximation of the posterior distribution. The particles themselves can be obtained from the `trace` object and the particle weights can be obtained from the  `res` object. \n",
    "\n",
    "Recall that the approximate posterior distribution is of the form $p(\\theta|\\mathcal{D}) = \\frac{1}{N}\\sum_{j=1}^{N} \\delta(\\theta - \\theta_j)$ (the code renormalizes the particles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_w = trace.w\n",
    "particles_alpha = trace.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SMC can approximate the model evidence it provides a principled way of comparing models. Let's compare 5 different polynomial regression models where we change the degree of the polynomial from 1 to 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the evidence for the various degrees\n",
    "log_Zs = []\n",
    "D = [1, 2, 3, 4, 5]\n",
    "for d in D:\n",
    "    phi = PolynomialBasis(d)\n",
    "    smc_model, trace = fit_poly(phi, X, y, num_particles=500)\n",
    "    log_Z = np.mean(trace.report.log_marginal_likelihood)\n",
    "    log_Zs.append(log_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, log_Z in zip(D, log_Zs):\n",
    "    print('- degree %d gives log Z = %.4f'%(d, log_Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "_=plt.bar(D, log_Zs, width=0.3)\n",
    "_=plt.xticks(D)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Polynomial degree', fontsize=20)\n",
    "plt.ylabel('Model Evidence', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "+ The model with degree 3 polynomials has the gradest evidence. However, the degree 4 and 5 seem also very plausible. Is this a problem for the theory of Bayesian model selection? What complicates things here, is that model 3 is included in model 4 which is included in model 5. This requires us to design special priors for the models being right. They have to be consistent in some sense. For example, if model 3 is right then model 4 must be right, etc.\n",
    "\n",
    "+ Revisit the motorcycle dataset problem. Evaluate the model evidence for a 1) Polynomial basis; 2) a Fourier basis; and 3) a Radial basis function basis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
