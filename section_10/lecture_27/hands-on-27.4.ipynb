{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pymc3 --upgrade\n",
    "!pip install arziv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Running on ArviZ v{}'.format(az.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 27.4 - Gibbs Sampling\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Combine transition kernels using (approximate) Gibbs sampling to sample from complicated joint distributions.\n",
    "\n",
    "**Notes:** \n",
    "- We are not going to do Gibbs sampling by hand. Gibbs sampling is behind the scenes in PyMC3.\n",
    "So, we are just going to use PyMC to sample from the postrior of some somewhat complicated models.\n",
    "- The first version of this notebook was compiled by [Dr. Rohit Tripathy](https://rohittripathy.netlify.app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ To introduce probabilistic programming as a flexible paradigm for data-driven inference.\n",
    "+ Introduce `PyMC3` - a popular probabilistic programming library. \n",
    "+ Demonstrate key features of the `PyMC3` API and demonstrate it's use with some simple examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Example 1 - Coal Mining Disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on Coal Mining disaster dataset. Consider the following time series dataset of recorded coal mining disasters in the UK from 1851 to 1962. Let us first import this dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/coal_mining_disasters.csv'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "disaster_data = pd.read_csv('coal_mining_disasters.csv')\n",
    "disaster_data.dropna()\n",
    "disasters = disaster_data.disasters.values\n",
    "years = disaster_data.year.values\n",
    "disaster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(disaster_data.year, disaster_data.disasters, 'o', markersize=10)\n",
    "plt.xlabel('Year', fontsize=15)\n",
    "plt.ylabel('Number of disasters', fontsize=15)\n",
    "plt.title('Recorded coal mining disasters in the UK.', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "bp = plt.bar(disaster_data.year, disaster_data.disasters, width=1.)\n",
    "plt.xlabel(\"Year\", fontsize=15)\n",
    "plt.ylabel(\"Number of disasters\", fontsize=15)\n",
    "plt.title(\"Recorded coal mining disasters in the UK.\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. How can we represent this disaster time series data? What are quantities of interests?\n",
    "2. Is 'disasters' variable categorial or continous? Can it be negative? What are some other constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset \n",
    "\n",
    "Occurrences of disasters in the time series is thought to be derived from a Poisson process with a large rate parameter in the early part of the time series (more disasters in early years), and from one with a smaller rate in the later part (less number of disasters in later years). We are interested in locating the change point in the series, which might be because of changes in mining safety regulations in latter years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Approach\n",
    "\n",
    "How are we going to develop model for this data? A good starting point would be to think about *how this data might be generated?* Try to imagine how you would recreate the dataset. We begin by asking how our observations might have been generated.\n",
    "\n",
    "1. We start by thinking \"what is the best random variable to describe this count data?\" A Poisson random variable is a good candidate because it can represent count data. So we model the number of coal mining related disasters as sampled from a Poisson distribution.\n",
    "\n",
    "2. Next, we think, \"Ok, assuming number of disasters are Poisson-distributed, what do I need for the Poisson distribution?\" Well, the Poisson distribution has a rate parameter $\\lambda$. \n",
    "\n",
    "3.  Do we know $\\lambda$? No. In fact, we have a suspicion that there are *two* $\\lambda$ values, one for the earlier years and one for the later years. We don't know when the change in this rate parameter occurs though, but call the switchpoint $\\tau$.\n",
    "\n",
    "4. What is a good distribution for the two $\\lambda$s? The exponential is good, as it assigns probabilities to positive real numbers. Well the exponential distribution has a parameter too, call it $\\alpha$.\n",
    "\n",
    "5.  Do we know what the parameter $\\alpha$ might be? No. At this point, we could continue and assign a distribution to $\\alpha$, but it's better to stop once we reach a set level of ignorance: whereas we have a prior belief about $\\lambda$, (\"it probably changes over time\", \"it's likely between 1 and 3\", etc.), we don't really have any strong beliefs about $\\alpha$. So it's best to stop here. \n",
    "\n",
    "    What is a good value for $\\alpha$ then? We think that the $\\lambda$s are between 1-3, so if we set $\\alpha$ really low (which corresponds to larger probability on high values) we are not reflecting our prior well. Similar, a too-high alpha misses our prior belief as well. A good idea for $\\alpha$ as to reflect our belief is to set the value so that the mean of $\\lambda$, given $\\alpha$, is equal to our observed mean. \n",
    "\n",
    "6. We have no expert opinion of when $\\tau$ might have occurred. So we will suppose $\\tau$ is from a discrete uniform distribution over the entire timespan.\n",
    "\n",
    "Here's a graphical model describing the relationships between the variables in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "gcp = Digraph('coal_mining_disasters_model')\n",
    "\n",
    "# define the nodes \n",
    "gcp.node('alpha', label='<&alpha;>', style='filled')\n",
    "gcp.node('tau', label='<&tau;>')\n",
    "gcp.node('lambda_1', label='<&lambda;<sub>1</sub>>')\n",
    "gcp.node('lambda_2', label='<&lambda;<sub>2</sub>>')\n",
    "gcp.node('lambda', label='<&lambda;>')\n",
    "gcp.node('tau', label='<&tau;>')\n",
    "gcp.node('obs', label='obs', style='filled')\n",
    "\n",
    "# define the edges \n",
    "gcp.edge('alpha', 'lambda_1')\n",
    "gcp.edge('alpha', 'lambda_2')\n",
    "gcp.edge('tau', 'lambda')\n",
    "gcp.edge('lambda_1', 'lambda')\n",
    "gcp.edge('lambda_2', 'lambda')\n",
    "gcp.edge('lambda', 'obs')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, the generative model is expressed as:\n",
    "\n",
    "$$\\lambda_1 \\sim \\mathrm{Exp}(\\lambda_1 | \\alpha),$$\n",
    "$$\\lambda_2 \\sim \\mathrm{Exp}(\\lambda_2 | \\alpha),$$\n",
    "$$ \\tau \\sim \\mathrm{DiscreteUniform}([1851, 1852, \\dots, 1961]), $$\n",
    "$$\\lambda = \n",
    "\\begin{cases}\n",
    "\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}$$\n",
    "$$\\mathrm{obs}_i \\sim \\mathrm{Poisson}(\\lambda_i)$$\n",
    "\n",
    "We will set the rate parameter $\\alpha$ on the exponential priors on $\\lambda_1$ and $\\lambda_2$ as a constant. \n",
    "The latent variables to be inferred are $\\lambda_1, \\lambda_2, \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyMC3` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_model = pm.Model()\n",
    "lower_year, upper_year = years.min(), years.max()\n",
    "alpha = 1.\n",
    "\n",
    "with disaster_model:\n",
    "    # define the prior \n",
    "    lambda_1 = pm.Exponential(\"lambda_1\", lam=alpha)\n",
    "    lambda_2 = pm.Exponential(\"lambda_2\", lam=alpha)\n",
    "    tau      = pm.DiscreteUniform('tau', lower=lower_year, upper=upper_year)\n",
    "    lmbda = pm.Deterministic('lambda', tt.switch(tau >= years, lambda_1, lambda_2))\n",
    "    \n",
    "    # define the likelihood \n",
    "    x = pm.Poisson('x', lmbda, observed=disasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and posterior visualization\n",
    "\n",
    "Now let's infer the hidden parameters with MCMC. \n",
    "The Gibbs sampler is operating on the background along with a different Metropolis-Hastings algorithm associated with each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    trace = pm.sample(draws=40000, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.traceplot(trace, var_names=['tau', 'lambda_1', 'lambda_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_posterior(trace, var_names=['tau', 'lambda_1', 'lambda_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_autocorr(trace, var_names=['tau','lambda_1', 'lambda_2'], combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_trace = trace[10000::10]\n",
    "_=pm.plot_autocorr(thinned_trace, var_names=['tau', 'lambda_1', 'lambda_2'], combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.traceplot(thinned_trace, var_names=['tau', 'lambda_1', 'lambda_2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the samples from the posterior of the unknown parameters in this model to estimate the expected number of disasters between $t = 1851$ and $t=1962$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppsamples = pm.sample_posterior_predictive(samples=2000, \n",
    "                                           model=disaster_model, \n",
    "                                           trace=thinned_trace)['x']\n",
    "plt.figure(figsize=(12, 8))\n",
    "bp = plt.bar(disaster_data.year, disaster_data.disasters, width=1.)\n",
    "plt.plot(disaster_data.year, ppsamples.mean(axis=0), 'ro', \n",
    "         markersize=10, label='Posterior predictive expectation')\n",
    "plt.xlabel(\"Year\", fontsize=15)\n",
    "plt.ylabel(\"Number of disasters\", fontsize=15)\n",
    "plt.title(\"Recorded coal mining disasters in the UK.\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(loc='best', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Example 2 - Challenger Space Shuttle Disaster\n",
    "\n",
    "For this tutorial, we are going to work on Challenger space shuttle disaster dataset.\n",
    "We have revisited the problem in [HW 5](https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/homework/homework_05.ipynb). There, we used logistic regression and we trained it with maximum likelihood.\n",
    "We can now follow a Bayesian approach to the fullest extent.\n",
    "\n",
    "On January 28, 1986, the twenty-fifth flight of the U.S. space shuttle program ended in disaster when one of the rocket boosters of the Shuttle Challenger exploded shortly after lift-off, killing all seven crew members. The presidential commission on the accident concluded that it was caused by the failure of an O-ring in a field joint on the rocket booster, and that this failure was due to a faulty design that made the O-ring unacceptably sensitive to a number of factors including outside temperature. Of the previous 24 flights, data were available on failures of O-rings on 23, (one was lost at sea), and these data were discussed on the evening preceding the Challenger launch, but unfortunately only the data corresponding to the 7 flights on which there was a damage incident were considered important and these were thought to show no obvious trend. The data are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/challenger_data.csv'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_data = np.genfromtxt(\"challenger_data.csv\", skip_header=1,\n",
    "                                usecols=[1, 2], missing_values=\"NA\",\n",
    "                                delimiter=\",\")\n",
    "challenger_data = challenger_data[~np.isnan(challenger_data[:, 1])]\n",
    "print(\"Temp (F), O-Ring failure?\")\n",
    "print(challenger_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it, as a function of temperature (the first column)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(challenger_data[:, 0], challenger_data[:, 1], 'ro', \n",
    "         markersize=15)\n",
    "plt.ylabel(\"Damage Incident?\",fontsize=20)\n",
    "plt.xlabel(\"Outside temperature (Fahrenheit)\",fontsize=20)\n",
    "plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\",\n",
    "          fontsize=20)\n",
    "plt.yticks([0, 1], fontsize=15)\n",
    "plt.xticks(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset \n",
    "It looks clear that *the probability* of damage incidents occurring increases as the outside temperature decreases. We are interested in modeling the probability here because it does not look like there is a strict cutoff point between temperature and a damage incident occurring. The best we can do is ask \"At temperature $t$, what is the probability of a damage incident?\". The goal of this example is to answer that question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. How can we represent this disaster binary 1/0 data? What are quantities of interests?\n",
    "2. Is 'damage incident' variable categorial or continous? What are some other constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling approach\n",
    "\n",
    "How are we going to develop model for this data? A good starting point would be to think about *how this data might be generated?* Try to imagine how you would recreate the dataset. We begin by asking how our observations might have been generated.\n",
    "\n",
    "- We start by thinking \"what is the best random variable to describe this binary categorical data?\" A Bernoulli random variable is a good candidate because it can represent binary data. So we model the variable of 'damage incident' as sampled from a Bernoulli distribution. A *Bernoulli* random variable with parameter $p$, denoted $\\text{Ber}(p)$, is a random variable that takes value 1 with probability $p$, and 0 else. Thus, our model can look like:\n",
    "$$ \\text{Defect Incident, $D_i$} \\sim \\text{Ber}( \\;p_i\\; ), \\;\\; i=1..N$$\n",
    "\n",
    "- Next, we think, \"Ok, assuming damage incident variable is Bernoulli-distributed, what do I need for the Bernoulli distribution?\" Well, the Bernoulli distribution has a probability parameter $p_i$. \n",
    "\n",
    "-  Do we know $p_i$ parameter? No. But we have a suspicion (intuition) that this parameter is dependent on outside temperature values. Less the value of outside temperature, more is the probability of damage incident. More the value of outside temperature, less is the probability of damage incident. With slight abuse in notation, we can define this as:\n",
    "$$p_i = p(t_i) = \\sigma(t_i),$$\n",
    "\n",
    "where $\\sigma: \\mathbb{R} \\rightarrow (0, 1)$ is a suitable function that maps arbitrary temperature values to the interval $(0, 1)$ (so that we can then interpret the output of $\\sigma$ as a probability). For this problem, we are going to use logistic link function which is given as:\n",
    "$$\\sigma(t) = \\frac{1}{ 1 + e^{ \\;\\beta t + \\alpha } } $$\n",
    "\n",
    "Some plots are shown below, with differing $\\alpha$ and $\\beta$ parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, beta, alpha=0):\n",
    "    return 1.0 / (1.0 + np.exp(np.dot(beta, x) + alpha))\n",
    "\n",
    "x = np.linspace(-4, 4, 100)\n",
    "plt.figure(figsize=(12, 5))\n",
    "alphas = [0, 0, 0, 1, 3, 5]\n",
    "betas  = [1, 3, -5, 1, 3, -5]\n",
    "params = zip(alphas, betas)\n",
    "for param in params:\n",
    "    alpha, beta = param\n",
    "    label=\"$\\\\alpha$ = %d, $\\\\beta$ = %d\"%(alpha, beta)\n",
    "    plt.plot(x, logistic(x, beta, alpha),\n",
    "             label=label, \n",
    "             linewidth=2.5)\n",
    "plt.title(\"Logistic functon with bias\", fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(loc=\"best\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "__How can we represent  $\\alpha$ and $\\beta$ parameter values? Are they categorical or continous? Do they need to be positive?__\n",
    "\n",
    "The $\\beta, \\alpha$ parameters have no reason to be positive, bounded or relatively large, so they are best modeled by a *Normal random variable*. Since we dont have any prior beliefs about the value of parameters beta or alpha, we place a vague prior Normal distribution (small precision, large variance) over their values.\n",
    "\n",
    "The graphical model for the data generation process is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp = Digraph('space_shuttle_disaster')\n",
    "\n",
    "# setup the nodes \n",
    "gcp.node('alpha', label='<&alpha;>')\n",
    "gcp.node('beta', label='<&beta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('pi', label='<p<sub>i</sub>>')\n",
    "    sg.node('xi', label='<x<sub>i</sub>>', style='filled')\n",
    "    sg.attr(color='blue')\n",
    "    sg.attr(label='i=1,2...')\n",
    "    sg.attr(labelloc='b')\n",
    "\n",
    "# setup the edges \n",
    "gcp.edge('alpha', 'pi')\n",
    "gcp.edge('beta', 'pi')\n",
    "gcp.edge('pi', 'xi')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyMC3` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the data and apply preprocessing if any \n",
    "temp = challenger_data[:, 0]\n",
    "temp_scaled = (temp - np.mean(temp))/np.std(temp)\n",
    "data = challenger_data[:, 1]\n",
    "\n",
    "# instantiate the pymc3 model\n",
    "challenger_model = pm.Model()\n",
    "\n",
    "# define the graph \n",
    "with challenger_model:\n",
    "    # define the prior\n",
    "    alpha = pm.Normal('alpha', mu=0., sigma=10.)\n",
    "    beta = pm.Normal('beta', mu=0., sigma=10.)\n",
    "    \n",
    "    # get the probabilities of failure at each observed temp \n",
    "    p = pm.Deterministic('p', 1./(1. + tt.exp(alpha + beta*temp_scaled)))\n",
    "    \n",
    "    # define the likelihood \n",
    "    x = pm.Bernoulli('x', p=p, observed=data)\n",
    "print(\"Challenger space shuttle disaster model:\")\n",
    "challenger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and posterior visualization\n",
    "\n",
    "Now let's infer the hidden parameters with MCMC. We will use the default NUTS sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with challenger_model:\n",
    "    trace = pm.sample(draws=40000, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var_names=['alpha', 'beta']\n",
    "pm.traceplot(trace, var_names=var_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_posterior(trace, var_names=var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_autocorr(trace, var_names=var_names, figsize=(12, 8), combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_trace = trace[10000::10]\n",
    "_=pm.autocorrplot(thinned_trace, var_names=var_names, figsize=(12, 8), combined=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictive distribution\n",
    "\n",
    "We can use the samples of posterior generated from MCMC to get a predictive over the probability of failure at any given temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with challenger_model:\n",
    "    ppsamples = pm.sample_posterior_predictive(trace=thinned_trace, \n",
    "                                               samples=2000,\n",
    "                                               var_names=['p'])['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posterior predictive mean and 95% interval\n",
    "ppmean = ppsamples.mean(axis=0)\n",
    "pp_lower, pp_upper = np.percentile(ppsamples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(temp, data, 'ro', markersize=12, label='Observed data')\n",
    "idx=np.argsort(temp)\n",
    "plt.plot(temp[idx], ppmean[idx], linestyle='--', linewidth=2.5, \n",
    "         label='Post. pred. mean prob.')\n",
    "plt.fill_between(temp[idx], pp_lower[idx], pp_upper[idx], \n",
    "                color='purple', alpha=0.25, label='95% Confidence')\n",
    "plt.ylabel(\"Probability estimate\",fontsize=20)\n",
    "plt.xlabel(\"Outside temperature (Fahrenheit)\",fontsize=20)\n",
    "plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\",\n",
    "          fontsize=20)\n",
    "plt.yticks(np.arange(0., 1.01, 0.2), fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *95% credible interval*, or 95% CI, painted in purple, represents the interval, for each temperature, that contains 95% of the distribution. For example, at 65 degrees, we can be 95% sure that the probability of defect lies between 0.25 and 0.75.\n",
    "\n",
    "More generally, we can see that as the temperature nears 60 degrees, the CI's spread out quickly. As we pass 70 degrees, the CI's tighten again. This can give us insight about how to proceed next: we should probably test more O-rings around 60-65 temperature to get a better estimate of probabilities in that range. Similarly, when reporting to scientists your estimates, you should be very cautious about simply telling them the expected probability, as we can see this does not reflect how *wide* the posterior distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "On the day of the Challenger disaster, the outside temperature was 31 degrees Fahrenheit. What is the posterior distribution of a defect occurring,  given this temperature? The distribution is plotted below. It looks almost guaranteed that the Challenger was going to be subject to defective O-rings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
