{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 27-28 (inclusive).\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - You can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope.\n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name:** Alex\n",
    "+ **Last Name:** Shank\n",
    "+ **Email:** shank14@purdue.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pymc3 --upgrade\n",
    "!pip install arziv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Running on ArviZ v{}'.format(az.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1  - Bayesian Linear regression\n",
    "\n",
    "Recall that the standard Bayesian Linear regression model admits closed form expressions for the posterior distribution over the weights and the posterior predictive distribution over the observations. Suppose you select a vector of suitable basis functions $\\phi(x) = (\\phi_1(x), \\phi_2(x) \\dots, \\phi_M(x))^T$, the predictive model is the GLM:\n",
    "$$\n",
    "f(x) = \\mathbf{w}^T \\phi(x).\n",
    "$$\n",
    "\n",
    "If weights are equipped with a Gaussian prior $p(\\mathbf{w}) \\sim \\mathcal{N}(\\mathbf{w}|0, \\Sigma_p)$ and the observations are assumed to follow a Gaussian distribution - $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{y} | \\Phi \\mathbf{w}, \\sigma^2 I)$, the posterior distribution over the weights is given by - $p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}) = \\mathcal{N}(\\mathbf{w} | \\mathbf{m}, \\mathbf{S})$, where, $\\mathbf{S} = (\\sigma^{-2} \\Phi^T \\Phi + \\Sigma_p )^{-1}$ and $\\mathbf{m} = \\sigma^{-2} \\mathbf{S} \\Phi^T \\mathbf{y}$ and the posterior predictive distribution over the observations are given by $p(y^*|x^*, \\mathbf{X}, \\mathbf{y}) =  \\mathcal{N} ( y^* | \\mathbf{m}^T \\phi(x^*), \\phi(x^*)^T \\mathbf{S} \\phi(x^*) + \\sigma^2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a generic function that computes the design matrix\n",
    "def compute_design_matrix(X, phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    X   -  The observed inputs (1D array)\n",
    "    phi -  The basis functions.\n",
    "    \"\"\"\n",
    "    num_observations = X.shape[0]\n",
    "    num_basis = phi.num_basis\n",
    "    Phi = np.ndarray((num_observations, num_basis))\n",
    "    for i in range(num_observations):\n",
    "        Phi[i, :] = phi(X[i, :])\n",
    "    return Phi\n",
    "\n",
    "# Here is a class for the polynomials:\n",
    "class PolynomialBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    degree  -  The degree of the polynomial.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        self.num_basis = degree + 1\n",
    "    def __call__(self, x):\n",
    "        return np.array([x[0] ** i for i in range(self.degree + 1)])\n",
    "\n",
    "# Here is a class for the Fourier basis:\n",
    "class FourierBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    num_terms  -  The number of Fourier terms.\n",
    "    L          -  The period of the function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_terms, L):\n",
    "        self.num_terms = num_terms\n",
    "        self.L = L\n",
    "        self.num_basis = 2 * num_terms\n",
    "    def __call__(self, x):\n",
    "        res = np.ndarray((self.num_basis,))\n",
    "        for i in range(num_terms):\n",
    "            res[2 * i] = np.cos(2 * i * np.pi / self.L * x[0])\n",
    "            res[2 * i + 1] = np.sin(2 * (i+1) * np.pi / self.L * x[0])\n",
    "        return res\n",
    "\n",
    "# Here is a class for the polynomials:\n",
    "class RadialBasisFunctions(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the radial basis functions.\n",
    "    ell -  The assumed lengthscale.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, ell):\n",
    "        self.X = X\n",
    "        self.ell = ell\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        return np.exp(-.5 * (x - self.X) ** 2 / self.ell ** 2).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Compare MCMC to analytical solution\n",
    "\n",
    "Setup a GLM for the motorcycle data (loaded below), with an fixed precision prior on the weights and a constant likelihood noise, and use `PyMC3` to determine the posterior over the weights and the posterior predictive distribution at new test inputs. Compare your MCMC solution for the posterior with the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/motor.dat'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = np.loadtxt('motor.dat')\n",
    "X = data[:, 0][:, None]\n",
    "Y = data[:, 1]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We place a diagonal covariance prior on the weights:\n",
    "$$\n",
    "\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w} | 0, \\alpha^{-1}I).\n",
    "$$\n",
    "\n",
    "The observation (or likelihood) model is given by:\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(y_i|\\mathbf{w}^T \\phi(\\mathbf{x}), \\sigma^2).\n",
    "$$\n",
    "\n",
    "The hyperparameters, $\\alpha$ (prior precision of the weights )and $\\sigma$ (the likelihood noise standard deviation) are set to be constant. \n",
    "\n",
    "For our basis functions, we will use 20 radial basis functions with lengthscales of 2. All the basis function centers are spaced equally in the interval $[0, 60]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyperparameters \n",
    "sigma = 20. \n",
    "gamma = 30.\n",
    "alpha = (1./gamma)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the basis \n",
    "ell = 2.\n",
    "num_basis = 20\n",
    "Xc = np.linspace(0, 60, num_basis)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "Phi_s = theano.shared(Phi) #Phi_s is needed to share Phi with the model_A. Later we can switch Phi with Phi_test \n",
    "#to perform posterior predictive checks on the test data.\n",
    "\n",
    "# define the model\n",
    "model_A = pm.Model()\n",
    "with model_A:\n",
    "    w = #Your code here\n",
    "    ymean = #Your code here\n",
    "    y = #Your code here\n",
    "    trace_A = pm.sample(draws=5000, tune=1000, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace_A, var_names=['w']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the autocorrelation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_autocorr(trace_A,\n",
    "                   combined=True,\n",
    "                   var_names=['w'], figsize=(20, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.linspace(0., 60, 500)[:, None]\n",
    "Phitest = compute_design_matrix(Xtest, phi)\n",
    "Phi_s.set_value(Phitest)\n",
    "yppsamples = pm.sample_posterior_predictive(var_names=['ymean'], \n",
    "                                              samples=500,\n",
    "                                              model=model_A, \n",
    "                                              trace=trace_A)['ymean']\n",
    "\n",
    "yppmean = yppsamples.mean(0)\n",
    "ypplow, ypphigh = np.percentile(yppsamples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the true posterior over w \n",
    "A = np.dot(Phi.T, Phi) / sigma ** 2. + alpha * np.eye(Phi.shape[1])\n",
    "L = scipy.linalg.cho_factor(A)\n",
    "\n",
    "# You can refer to hand-on activity for lecture 14\n",
    "m = #Your code here  # The posterior mean of w\n",
    "S = #Your code here  # The posterior covariance of w\n",
    "\n",
    "# get the posterior predictive distribution\n",
    "Phi_p = compute_design_matrix(Xtest, phi)\n",
    "Y_p = np.dot(Phi_p, m) \n",
    "V_p = np.einsum('ij,jk,ik->i', Phi_p, S, Phi_p) + sigma ** 2 \n",
    "S_p =np.sqrt(V_p)\n",
    "Y_l = Y_p - 2. * S_p \n",
    "Y_u = Y_p + 2. * S_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.plot(Xtest[:, 0], yppmean,label='Post. pred. mean')\n",
    "plt.fill_between(Xtest[:,0], ypplow, ypphigh, alpha=0.25, label='Post. pred. CI')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('MCMC Posterior Predictive Distribution', fontsize=15)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.plot(Xtest[:, 0], Y_p,label='Post. pred. mean')\n",
    "plt.fill_between(Xtest[:,0], Y_l, Y_u, alpha=0.25, label='Post. pred. CI')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Analytical Posterior Predictive Distribution', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Hierarchical Priors \n",
    "\n",
    "Specify priors on the model hyperparameters and estimate the full joint posterior over the model weights and model hyperparameters. At a minimum, specify appropriate priors for the  prior precision of the weights vector, and the likelihood noise. Use the estimated posterior to get the posterior predictive distribution over test inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We will set the following priors on $\\alpha$ and $\\sigma$ - \n",
    "$$\n",
    "\\alpha \\sim \\mathrm{Exp}(\\alpha | 1), \\\\\n",
    "\\sigma \\sim \\mathrm{Exp}(\\sigma | 1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_s.set_value(Phi)\n",
    "model_B = pm.Model()\n",
    "\n",
    "with model_B:\n",
    "    alpha = #your code here  # seperate precision for each weight \n",
    "    w = #your code here\n",
    "    sigma = #your code here\n",
    "    ymean = #your code here\n",
    "    y = #your code here\n",
    "    trace_B = pm.sample(draws=5000, tune=1000, progressbar=True, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior over the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior over the noise parameters (alpha, sigma):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelations in the posterior samples of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelations in the posterior samples of the noise parameters (alpha, sigma):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior predictive distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_s.set_value(Phitest)\n",
    "yppsamples = pm.sample_posterior_predictive(var_names=['ymean'], \n",
    "                                              samples=500,\n",
    "                                              model=model_B, \n",
    "                                              trace=trace_B)['ymean']\n",
    "\n",
    "yppmean = yppsamples.mean(0)\n",
    "ypplow, ypphigh = np.percentile(yppsamples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.plot(Xtest[:, 0], yppmean,label='Post. pred. mean')\n",
    "plt.fill_between(Xtest[:,0], ypplow, ypphigh, alpha=0.25, label='Post. pred. CI')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('Posterior Predictive Distribution', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C - Heteroscedastic regression\n",
    "\n",
    "So far, throughout this course, you have seen likelihood models that assume that the noise level $\\sigma$ is independent of the input $x$. This is known as *homoscedasticity* - the assumption that errors in a regression model are indepedent of the inputs. Consider the following likelihood model with input dependent noise:\n",
    "$$\n",
    "y \\sim \\mathcal{N} ( y | \\mathbf{w}^T \\phi(x) , \\sigma(x)^2),\n",
    "$$\n",
    "where the likelihood noise depends on the input. \n",
    "Approximate $\\log \\sigma$ as a GLM of your choice - $\\log \\sigma = \\phi_{\\sigma}(x)^T \\mathbf{w_{\\sigma}}$ to model the dependence of the likelihood noise to the input. \n",
    "Develop the `PyMC3` model to express the heteroscedastic model and estimate the joint posterior over all parameters and hyperparameters. \n",
    "The parameters that you need to infer will include the weights of the output GLM $\\mathbf{w}$, the weights of the noise GLM model $\\mathbf{w}_{\\sigma}$, the precision over $\\mathbf{w}$ and the precision over $\\mathbf{w}_{\\sigma}$ and any additional hyperparameters you might have in your model.\n",
    "Use the estimated posterior to show the posterior predictive distribution over test inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We will model the log of the likelihood standard deviation with a GLM:\n",
    "$$\n",
    "\\log \\sigma = \\phi_{\\sigma}(x)^T \\mathbf{w_{\\sigma}}.\n",
    "$$\n",
    "\n",
    "To keep things simple, we will simply use the same basis functions being used for the GLM of the response, $y$. \n",
    "\n",
    "The hierarchical model we will use is as follows. Note that we place a prior over the lengthscale, $\\ell$, of the RBF basis functions also:\n",
    "$$\n",
    "\\ell \\sim \\mathrm{Exp}(\\ell | 1), \\\\\n",
    "\\alpha_i \\sim \\mathrm{Exp}(\\alpha | 1), \\\\\n",
    "\\alpha_{\\sigma, i} \\mathrm{Exp}(\\alpha | 1), \\\\\n",
    "\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w} | 0, \\boldsymbol{\\alpha}^{-1}), \\\\\n",
    "\\mathbf{w}_{\\sigma} \\sim \\mathcal{N}(\\mathbf{w}_{\\sigma} | 0, \\boldsymbol{\\alpha_{\\sigma}}^{-1}), \\\\\n",
    "y_i \\sim \\mathcal{N}\\left(y_i | \\mathbf{w}^T \\phi(x), \\exp \\left( \\mathbf{w}_{\\sigma}^T \\phi(x) \\right)^2 \\right),\n",
    "$$\n",
    "where, $\\boldsymbol{\\alpha} = \\mathrm{diag}(\\alpha_1, \\alpha_2, \\dots, \\alpha_D)$, i.e., a diagonal matrix whose diagonal elements are the precisions of the individual weights in the GLM, and $\\boldsymbol{\\alpha}_{\\sigma} = \\mathrm{diag}(\\alpha_{\\sigma, 1}, \\alpha_{\\sigma, 2}, \\dots, \\alpha_{\\sigma, D})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Model with Heteroscedastic Regression\n",
    "num_basis = 20\n",
    "Xc = np.linspace(0, 60, num_basis)\n",
    "Xcs = theano.shared(Xc[None, :], broadcastable=(True, False))\n",
    "Xs = theano.shared(X, broadcastable=(False,True))\n",
    "model_C = pm.Model()\n",
    "with model_C:\n",
    "    # prior over the lengthscales\n",
    "    ell = #your code here\n",
    "    \n",
    "    # design matrix \n",
    "    Phi = pm.Deterministic('Phi', pm.math.exp(-0.5 * ((Xs-Xcs)**2.0)/(ell**2)))\n",
    "    \n",
    "    # prior over weight precision \n",
    "    alpha = #your code here\n",
    "    alpha_sigma = #your code here\n",
    "    \n",
    "    # weights \n",
    "    w = #your code here\n",
    "    w_sigma = #your code here\n",
    "    \n",
    "    # GLMs for data and noise \n",
    "    ymean = #your code here\n",
    "    sigma = #your code here\n",
    "    \n",
    "    # Likelihood \n",
    "    y = #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_C:\n",
    "    trace_C = pm.sample(draws=2000, tune=500, progressbar=True, step=pm.NUTS(target_accept=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the posterior. First, let's look at the posterior over the lengthscale parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out the posterior over the precision parameters, $\\alpha_i$s and $\\alpha_{\\sigma, i}$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the posterior over the weights (w and w_sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visually inspect the autocorrelation for lengthscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the autocorrelation for $\\alpha_i$s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the autocorrelation for $\\alpha_{\\sigma, i}$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the autocorrelation for the weights (w and w_sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the autocorrelation, let's thin the trace by taking every 5th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_trace_C = trace_C[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Sampling\n",
    "Xtest = np.linspace(0., 60., 300)[:, None]\n",
    "Xs.set_value(Xtest)\n",
    "with model_C:\n",
    "    post_samples_C = pm.sample_posterior_predictive(trace=thin_trace_C, samples=100, var_names=['ymean', 'sigma', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some samples from the mean process and the noise process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "for i in range(10):\n",
    "    ax[0].plot(Xtest[:, 0], post_samples_C['ymean'][i, :])\n",
    "    ax[1].plot(Xtest[:, 0], post_samples_C['sigma'][i, :])\n",
    "ax[0].plot(X, Y, 'ro', label='Data')\n",
    "ax[0].set_title('Samples of the mean')\n",
    "ax[1].set_title('Samples of the noise')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the full posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples = post_samples_C['y']\n",
    "ymean_samples = post_samples_C['ymean']\n",
    "ypostmean = ymean_samples.mean(0)\n",
    "y_samples_low, y_samples_high = np.percentile(y_samples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.plot(Xtest[:,0], ypostmean, linewidth=2.5, label='Mean')\n",
    "plt.fill_between(Xtest[:,0], y_samples_low, y_samples_high, alpha=0.25, label='Post. pred. CI')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2  - Bayesian Linear regression: Sequential Monte Carlo (SMC)\n",
    "\n",
    "In this problem you need to to demonstrate how you can use Sequential Monte Carlo (SMC) using `PyMC3`. You need to re-run and compare the GLM and the model with hierarchical priors that you created in Problem 1 using SMC.\n",
    "\n",
    "For demonstration purpose I have created the first model (GLM with an fixed precision prior on the weights and a constant likelihood noise) for you. \n",
    "\n",
    "For the model with hierarchical priors,you will have to:\n",
    "\n",
    "1) Setup the model with hierarchical priors.\n",
    "\n",
    "2) Once you have the trace object for the SMC simulation apply all the standard postprocessing tools from PyMC3 as usual. Make sure to reduce the autocorrelation, if necessary, by thinning the trace. \n",
    "\n",
    "3) Plot posterior predictive distribution.\n",
    "\n",
    "4) Find the model evidence.\n",
    "\n",
    "5) Finally, compare the two models based on model evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: GLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the hyperparameters \n",
    "sigma = 20.\n",
    "gamma = 30.\n",
    "alpha = (1./gamma)**2\n",
    "\n",
    "# define the basis \n",
    "ell = 2.\n",
    "num_basis = 20\n",
    "Xc = np.linspace(0, 60, num_basis)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "Phi_s = theano.shared(Phi)\n",
    "\n",
    "# define the model\n",
    "model_GLM_SMC = pm.Model()\n",
    "with model_GLM_SMC:\n",
    "    w = pm.Normal('w', 0., gamma, shape=phi.num_basis)\n",
    "    ymean = pm.Deterministic('ymean', tt.dot(Phi_s, w))\n",
    "    y = pm.Normal('y', ymean, sigma=sigma, observed=Y)\n",
    "    trace_GLM_SMC = pm.sample_smc(draws=num_particles, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior over the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace_GLM_SMC, var_names=['w']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pm.plot_autocorr(trace_GLM_SMC,\n",
    "                   combined=True,\n",
    "                   var_names=['w'], figsize=(20, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation plots looks good. No need for thinning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.linspace(0., 60, 500)[:, None]\n",
    "Phitest = compute_design_matrix(Xtest, phi)\n",
    "Phi_s.set_value(Phitest)\n",
    "yppsamples = pm.sample_posterior_predictive(var_names=['ymean'], \n",
    "                                              samples=500,\n",
    "                                              model=model_GLM_SMC, \n",
    "                                              trace=trace_GLM_SMC)['ymean']\n",
    "yppmean = yppsamples.mean(0)\n",
    "ypplow, ypphigh = np.percentile(yppsamples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.plot(X, Y, 'ro', label='Data')\n",
    "plt.plot(Xtest[:, 0], yppmean,label='Post. pred. mean')\n",
    "plt.fill_between(Xtest[:,0], ypplow, ypphigh, alpha=0.25, label='Post. pred. CI')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.title('SMC Posterior Predictive Distribution', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the model evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Z_SMC_GLM = np.mean(trace_GLM_SMC.report.log_marginal_likelihood)\n",
    "print('log Z (SMC_GLM) = {0:.3f}'.format(log_Z_SMC_GLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Hierarchical Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot posterior over the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot posterior over the alpha and sigma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot autocorrelation for the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot autocorrelation for alpha and sigma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the model evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which of the above two models is better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
